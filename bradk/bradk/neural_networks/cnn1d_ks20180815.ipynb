{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import math\n",
    "#-------------------------------\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_test, y_test, hpar):\n",
    "    input_shape = x_train.shape[1:]\n",
    "    output_num = y_train.shape[1]\n",
    "    if hpar['layer_activ_function_at_output'] == 'sigmoid': # loss function\n",
    "        loss_func = 'binary_crossentropy'\n",
    "    elif hpar['layer_activ_function_at_output'] == 'softmax':\n",
    "        loss_func = 'categorical_crossentropy'\n",
    "    if hpar['optimizer'] == 'gradient descent': # optimizer\n",
    "        optimizer = SGD(lr=hpar['learning_rate'])\n",
    "    elif hpar['optimizer'] == 'adam':\n",
    "        optimizer = Adam(lr=hpar['learning_rate'])\n",
    "    #-----------------------------------------------------\n",
    "    model = Sequential()\n",
    "    model.add(Dropout(hpar['layer_dropout_rate_at_input'], input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    for i in range(0, len(hpar['layer_conv_filter'])):\n",
    "        conv_ksize = hpar['layer_conv_filter'][i][0]\n",
    "        conv_filters = hpar['layer_conv_filter'][i][1]\n",
    "        conv_stride = hpar['layer_conv_stride'][i]\n",
    "        conv_pad = hpar['layer_conv_padding'][i]\n",
    "        pool_ksize = hpar['layer_pool_filter'][i]\n",
    "        pool_stride = hpar['layer_pool_stride'][i]\n",
    "        pool_pad = hpar['layer_pool_padding'][i]\n",
    "        model.add(Conv1D(conv_filters, conv_ksize, strides=conv_stride, padding=conv_pad))\n",
    "        model.add(Dropout(hpar['layer_dropout_rate_at_conv'][i]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(hpar['layer_activ_function_at_conv'])) # 'sigmoid', 'tanh', 'relu'\n",
    "        model.add(MaxPooling1D(pool_size=pool_ksize, strides=pool_stride, padding=pool_pad))\n",
    "    model.add(Flatten())\n",
    "    for i in range(len(hpar['layer_fc'])):\n",
    "        model.add(Dense(hpar['layer_fc'][i]))\n",
    "        model.add(Dropout(hpar['layer_dropout_rate_at_fc'][i]))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Activation(hpar['layer_activ_function_at_fc'])) # 'sigmoid', 'tanh', 'relu'\n",
    "    model.add(Dense(output_num))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation(hpar['layer_activ_function_at_output']))\n",
    "    model.compile(optimizer = optimizer, loss = loss_func, metrics = ['accuracy'])\n",
    "    #-----------------------------------------------------\n",
    "    model.summary()\n",
    "    loss = []\n",
    "    accu = []\n",
    "    for i in range(1, 1 + max(1, hpar['num_epochs'] // hpar['print_cost'])):\n",
    "        tic = time.time()\n",
    "        fout = model.fit(x_train, y_train, epochs=hpar['print_cost'], batch_size=hpar['minibatch_size'], verbose=0)\n",
    "        toc = time.time()\n",
    "        loss = loss + fout.history['loss'] # training set\n",
    "        accu = accu + fout.history['acc'] # training set\n",
    "        score_train = model.evaluate(x_train, y_train, batch_size=hpar['minibatch_size'], verbose=0) # score = [loss, acc]\n",
    "        score_test = model.evaluate(x_test, y_test, batch_size=hpar['minibatch_size'], verbose=0) # score = [loss, acc]\n",
    "        print('>> epochs = ', (i*hpar['print_cost']),'; Train loss/acc = %f/%f'%(score_train[0],score_train[1]), '; Test loss/acc = %f/%f'%(score_test[0],score_test[1]), '; time = %f sec '%(toc-tic))\n",
    "    mout = {'model': model,\n",
    "            'loss': loss,\n",
    "            'accu': accu}\n",
    "    return mout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    hpar = {}\n",
    "    hpar['layer_dropout_rate_at_input'] = 0.0\n",
    "    hpar['layer_conv_filter'] = [[7,8], [3,16]] # [height, width, channel]\n",
    "    hpar['layer_conv_stride'] = [1, 1] # [height, width]\n",
    "    hpar['layer_conv_padding'] = ['SAME', 'SAME']\n",
    "    hpar['layer_dropout_rate_at_conv'] = [0.0, 0.0]\n",
    "    hpar['layer_activ_function_at_conv'] = ['sigmoid', 'tanh', 'relu'][1]\n",
    "    hpar['layer_pool_filter'] = [4, 4] # [height, width]\n",
    "    hpar['layer_pool_stride'] = [4, 4] # [height, width]\n",
    "    hpar['layer_pool_padding'] = ['VALID', 'VALID']\n",
    "    hpar['layer_fc'] = []\n",
    "    hpar['layer_dropout_rate_at_fc'] = [0.0] # ex) [0.0, 0.0, 0.0]\n",
    "    hpar['layer_activ_function_at_fc'] = ['sigmoid', 'tanh', 'relu'][1]\n",
    "    hpar['layer_activ_function_at_output'] = ['sigmoid', 'softmax'][1]\n",
    "    hpar['learning_rate'] = 0.4\n",
    "    hpar['num_epochs'] = 3000\n",
    "    hpar['minibatch_size'] = 999999\n",
    "    hpar['optimizer'] = [\"gradient descent\", \"momentum\", \"adam\"][0]\n",
    "    hpar['print_cost'] = 100 # print epoch step; no print when it is zero\n",
    "    mout = train(x_train, y_train_one_hot, x_test, y_test_one_hot, hpar)\n",
    "    #-------------------------------------------------------------\n",
    "    plt.plot(mout['loss'])\n",
    "    plt.show()\n",
    "    plt.plot(mout['accu'])\n",
    "    plt.show()\n",
    "    #-------------------------------------------------------------\n",
    "    yp_test_one_hot = mout['model'].predict(x_test)\n",
    "    yp_test = np.argmax(yp_test_one_hot, axis = 1).reshape(y_test.shape)\n",
    "    accuracy_test = np.mean(y_test == yp_test)\n",
    "    print(accuracy_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nWhat's new from cnn1d_ks20180119?\\n(1) drop-out layer added\\n(2) some parameter names changed\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "What's new from cnn1d_ks20180119?\n",
    "(1) drop-out layer added\n",
    "(2) some parameter names changed\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
