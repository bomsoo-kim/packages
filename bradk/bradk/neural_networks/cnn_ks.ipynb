{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Model\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import layer_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "import pydot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "#from kt_utils import *\n",
    "\n",
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_last')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def HappyModel(input_shape, output_shape, hpar):\n",
    "    X_input = Input(input_shape, name = 'input0') # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
    "    X = X_input\n",
    "    L = len(hpar['layer_conv_filter'])\n",
    "    for i in range(0, L):\n",
    "        conv_ksize = tuple(hpar['layer_conv_filter'][i][0:2])\n",
    "        conv_filters = hpar['layer_conv_filter'][i][2]\n",
    "        conv_stride = tuple(hpar['layer_conv_stride'][i])\n",
    "        conv_pad = hpar['layer_conv_padding'][i]\n",
    "        pool_ksize = tuple(hpar['layer_pool_filter'][i])\n",
    "        pool_stride = tuple(hpar['layer_pool_stride'][i])\n",
    "        pool_pad = hpar['layer_pool_padding'][i]\n",
    "        X = Conv2D(conv_filters, conv_ksize, strides = conv_stride, padding = conv_pad, name = 'conv%d'%i)(X) # CONV -> BN -> RELU Block applied to X\n",
    "        X = BatchNormalization(axis = 3, name = 'bn%d'%i)(X) # CONV -> BN -> RELU Block applied to X\n",
    "        X = Activation('relu', name = 'activation%d'%i)(X) # CONV -> BN -> RELU Block applied to X\n",
    "        X = MaxPooling2D(pool_size = pool_ksize, strides = pool_stride, padding = pool_pad, name='max_pool%d'%i)(X) # MAXPOOL\n",
    "    X = Flatten(name = 'flatten')(X) # FLATTEN X (means convert it to a vector)\n",
    "    Lf = len(hpar['hlayer_fully_connected'])\n",
    "    for i in range(0, Lf):\n",
    "        num_units = hpar['hlayer_fully_connected'][i]\n",
    "        X = Dense(num_units, activation='relu', name='fc%d'%i)(X) # FULLYCONNECTED\n",
    "    X = Dense(output_shape, activation='sigmoid', name='fc%d'%Lf)(X) # FULLYCONNECTED\n",
    "    model = Model(inputs = X_input, outputs = X, name='HappyModel') # Create model. This creates your Keras model instance, you'll use this instance to train/test the model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 600\n",
      "number of test examples = 150\n",
      "X_train shape: (600, 64, 64, 3)\n",
      "Y_train shape: (600, 1)\n",
      "X_test shape: (150, 64, 64, 3)\n",
      "Y_test shape: (150, 1)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input0 (InputLayer)          (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv0 (Conv2D)               (None, 64, 64, 32)        4736      \n",
      "_________________________________________________________________\n",
      "bn0 (BatchNormalization)     (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation0 (Activation)     (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pool0 (MaxPooling2D)     (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "fc0 (Dense)                  (None, 1)                 32769     \n",
      "=================================================================\n",
      "Total params: 37,633\n",
      "Trainable params: 37,569\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "600/600 [==============================] - 22s 36ms/step - loss: 2.4775 - acc: 0.6250\n",
      "Epoch 2/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.2361 - acc: 0.9083\n",
      "Epoch 3/40\n",
      "600/600 [==============================] - 20s 33ms/step - loss: 0.2116 - acc: 0.9200\n",
      "Epoch 4/40\n",
      "600/600 [==============================] - 25s 42ms/step - loss: 0.2699 - acc: 0.9117\n",
      "Epoch 5/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.1613 - acc: 0.9233\n",
      "Epoch 6/40\n",
      "600/600 [==============================] - 18s 31ms/step - loss: 0.1417 - acc: 0.9517\n",
      "Epoch 7/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.1239 - acc: 0.9567\n",
      "Epoch 8/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1172 - acc: 0.9583\n",
      "Epoch 9/40\n",
      "600/600 [==============================] - 18s 31ms/step - loss: 0.1322 - acc: 0.9433\n",
      "Epoch 10/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.1106 - acc: 0.9667\n",
      "Epoch 11/40\n",
      "600/600 [==============================] - 18s 31ms/step - loss: 0.1260 - acc: 0.9500\n",
      "Epoch 12/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.1545 - acc: 0.9500\n",
      "Epoch 13/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1374 - acc: 0.9617\n",
      "Epoch 14/40\n",
      "600/600 [==============================] - 19s 31ms/step - loss: 0.0603 - acc: 0.9767\n",
      "Epoch 15/40\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.0783 - acc: 0.9667\n",
      "Epoch 16/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0819 - acc: 0.9783\n",
      "Epoch 17/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.1095 - acc: 0.9767\n",
      "Epoch 18/40\n",
      "600/600 [==============================] - 20s 33ms/step - loss: 0.0471 - acc: 0.9883\n",
      "Epoch 19/40\n",
      "600/600 [==============================] - 20s 33ms/step - loss: 0.0638 - acc: 0.9817\n",
      "Epoch 20/40\n",
      "600/600 [==============================] - 19s 31ms/step - loss: 0.0579 - acc: 0.9867\n",
      "Epoch 21/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.1410 - acc: 0.9517\n",
      "Epoch 22/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0864 - acc: 0.9633\n",
      "Epoch 23/40\n",
      "600/600 [==============================] - 21s 35ms/step - loss: 0.0668 - acc: 0.9783\n",
      "Epoch 24/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0889 - acc: 0.9700\n",
      "Epoch 25/40\n",
      "600/600 [==============================] - 18s 29ms/step - loss: 0.0510 - acc: 0.9783\n",
      "Epoch 26/40\n",
      "600/600 [==============================] - 20s 34ms/step - loss: 0.2058 - acc: 0.9433\n",
      "Epoch 27/40\n",
      "600/600 [==============================] - 21s 34ms/step - loss: 0.1925 - acc: 0.9500\n",
      "Epoch 28/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0442 - acc: 0.9883\n",
      "Epoch 29/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0511 - acc: 0.9817\n",
      "Epoch 30/40\n",
      "600/600 [==============================] - 18s 31ms/step - loss: 0.0655 - acc: 0.9783\n",
      "Epoch 31/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0549 - acc: 0.9850\n",
      "Epoch 32/40\n",
      "600/600 [==============================] - 19s 31ms/step - loss: 0.0380 - acc: 0.9950\n",
      "Epoch 33/40\n",
      "600/600 [==============================] - 21s 34ms/step - loss: 0.1829 - acc: 0.9550\n",
      "Epoch 34/40\n",
      "600/600 [==============================] - 22s 37ms/step - loss: 0.0562 - acc: 0.9850\n",
      "Epoch 35/40\n",
      "600/600 [==============================] - 19s 32ms/step - loss: 0.0612 - acc: 0.9867\n",
      "Epoch 36/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0441 - acc: 0.9900\n",
      "Epoch 37/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0422 - acc: 0.9883\n",
      "Epoch 38/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0498 - acc: 0.9850\n",
      "Epoch 39/40\n",
      "600/600 [==============================] - 18s 30ms/step - loss: 0.0174 - acc: 0.9933\n",
      "Epoch 40/40\n",
      "600/600 [==============================] - 18s 29ms/step - loss: 0.0252 - acc: 0.9933\n",
      "150/150 [==============================] - 2s 14ms/step\n",
      "\n",
      "Loss = 0.462895335356\n",
      "Test Accuracy = 0.873333334923\n"
     ]
    }
   ],
   "source": [
    "def load_HAPPY_data(dirpath = './datasets/coursera_dnn4_week2_Keras Tutorial - The Happy House/datasets/'):\n",
    "    import h5py\n",
    "    train_dataset = h5py.File(dirpath+'train_happy.h5', \"r\")\n",
    "    X_train_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    test_dataset = h5py.File(dirpath+'test_happy.h5', \"r\")\n",
    "    X_test_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    Y_train_orig = Y_train_orig.reshape((1, Y_train_orig.shape[0]))\n",
    "    Y_test_orig = Y_test_orig.reshape((1, Y_test_orig.shape[0]))\n",
    "    #-------------------------------------------------------\n",
    "    X_train = X_train_orig/255. # Normalize image vectors\n",
    "    X_test = X_test_orig/255.\n",
    "    Y_train = Y_train_orig.T # Reshape\n",
    "    Y_test = Y_test_orig.T\n",
    "    return X_train, Y_train, X_test, Y_test, classes\n",
    "if True:\n",
    "    X_train, Y_train, X_test, Y_test, classes = load_HAPPY_data()\n",
    "    print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "    print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "    print (\"X_train shape: \" + str(X_train.shape))\n",
    "    print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "    print (\"X_test shape: \" + str(X_test.shape))\n",
    "    print (\"Y_test shape: \" + str(Y_test.shape))\n",
    "    #-------------------------------------------------------\n",
    "    hpar = {}\n",
    "    hpar['layer_conv_filter'] = [[7,7,32]] # [height, width, channel]\n",
    "    hpar['layer_conv_stride'] = [[1,1]] # [height, width]\n",
    "    hpar['layer_conv_padding'] = ['SAME']\n",
    "    hpar['layer_pool_filter'] = [[2,2]] # [height, width]\n",
    "    hpar['layer_pool_stride'] = [[2,2]] # [height, width]\n",
    "    hpar['layer_pool_padding'] = ['VALID']\n",
    "    hpar['hlayer_fully_connected'] = []\n",
    "    #-------------------------------------------------------\n",
    "    model = HappyModel((64,64,3), 1, hpar)\n",
    "    model.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
    "    model.summary()\n",
    "    model.fit(x = X_train, y = Y_train, epochs = 40, batch_size = 16)\n",
    "    preds = model.evaluate(x = X_test, y = Y_test)\n",
    "    print()\n",
    "    print (\"Loss = \" + str(preds[0]))\n",
    "    print (\"Test Accuracy = \" + str(preds[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param #   \n",
    "=================================================================\n",
    "input_4 (InputLayer)         (None, 64, 64, 3)         0         \n",
    "_________________________________________________________________\n",
    "zero_padding2d_4 (ZeroPaddin (None, 70, 70, 3)         0         \n",
    "_________________________________________________________________\n",
    "conv0 (Conv2D)               (None, 64, 64, 32)        4736      \n",
    "_________________________________________________________________\n",
    "bn0 (BatchNormalization)     (None, 64, 64, 32)        128       \n",
    "_________________________________________________________________\n",
    "activation_4 (Activation)    (None, 64, 64, 32)        0         \n",
    "_________________________________________________________________\n",
    "max_pool (MaxPooling2D)      (None, 32, 32, 32)        0         \n",
    "_________________________________________________________________\n",
    "flatten_4 (Flatten)          (None, 32768)             0         \n",
    "_________________________________________________________________\n",
    "fc (Dense)                   (None, 1)                 32769     \n",
    "=================================================================\n",
    "Total params: 37,633\n",
    "Trainable params: 37,569\n",
    "Non-trainable params: 64\n",
    "\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
