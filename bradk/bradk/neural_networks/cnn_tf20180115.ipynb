{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_H0, n_W0, n_C0, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_H0, n_W0, n_C0)) # (batch, height, width, channel)\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, n_y)) # (batch,classes)\n",
    "    return X, Y\n",
    "def initialize_parameters(n_C0, hpar):\n",
    "    tf.set_random_seed(1)                              # so that your \"random\" numbers match ours\n",
    "    parameters = {}\n",
    "    n_C = n_C0\n",
    "    for i in range(1, len(hpar['layer_conv_filter'])+1):\n",
    "        n_H = hpar['layer_conv_filter'][i-1][0] # height\n",
    "        n_W = hpar['layer_conv_filter'][i-1][1] # width\n",
    "        n_F = hpar['layer_conv_filter'][i-1][2] # channel (the number of filter)\n",
    "        parameters['W%d'%i] = tf.get_variable('W%d'%i, [n_H, n_W, n_C, n_F], initializer = tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "        n_C = n_F # channel for the next element\n",
    "    return parameters\n",
    "def forward_propagation(X, n_y, parameters, hpar):\n",
    "    L = len(hpar['layer_conv_filter'])\n",
    "    units = {'P0': X}\n",
    "    for i in range(1, L+1): #CONV2D -> RELU -> MAXPOOL LAYERS\n",
    "        s_conv = [1] + hpar['layer_conv_stride'][i-1] + [1] # [1,s,s,1]\n",
    "        p_conv = hpar['layer_conv_padding'][i-1]\n",
    "        f_pool = [1] + hpar['layer_pool_filter'][i-1] + [1] # [1,f,f,1]\n",
    "        s_pool = [1] + hpar['layer_pool_stride'][i-1] + [1] # [1,s,s,1]\n",
    "        p_pool = hpar['layer_pool_padding'][i-1]\n",
    "        units['Z%d'%i] = tf.nn.conv2d(units['P%d'%(i-1)], parameters['W%d'%i], strides = s_conv, padding = p_conv) # CONV2D: stride of 1, padding 'SAME'\n",
    "        units['A%d'%i] = tf.nn.relu(units['Z%d'%i]) # RELU\n",
    "        units['P%d'%i] = tf.nn.max_pool(units['A%d'%i], ksize = f_pool, strides = s_pool, padding = p_pool) # MAXPOOL: window 8x8, sride 8, padding 'SAME'\n",
    "    Lf = len(hpar['hlayer_fully_connected'])\n",
    "    units['F0'] = tf.contrib.layers.flatten(units['P%d'%L]) # FLATTEN\n",
    "    for i in range(1, Lf + 1): # FULLY-CONNECTED LAYERS\n",
    "        num_FC = hpar['hlayer_fully_connected'][i-1]\n",
    "        units['F%d'%i] = tf.contrib.layers.fully_connected(units['F%d'%(i-1)], num_FC, activation_fn=tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer(seed=0))\n",
    "    units['F%d'%(Lf+1)] = tf.contrib.layers.fully_connected(units['F%d'%(Lf)], n_y, activation_fn=None, weights_initializer=tf.contrib.layers.xavier_initializer(seed=0)) # FULLY-CONNECTED without non-linear activation function (not not call softmax).\n",
    "    return units['F%d'%(Lf+1)], units\n",
    "def compute_cost(Z3, Y):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Z3, labels=Y))\n",
    "    return cost\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): # modified date: 01/15/2018\n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[permutation,:,:,:]\n",
    "    shuffled_Y = Y[permutation,:]\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = max(1, m // mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[k * mini_batch_size : k * mini_batch_size + mini_batch_size,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    if (m >= mini_batch_size) and (m % mini_batch_size != 0): # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[num_complete_minibatches * mini_batch_size : m,:,:,:]\n",
    "        mini_batch_Y = shuffled_Y[num_complete_minibatches * mini_batch_size : m,:]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "def print_units(parameters, units, hpar):\n",
    "    for i in range(1, len(hpar['layer_conv_filter'])+1):\n",
    "        print('>> param shape = ', parameters['W%d'%i])\n",
    "    print('>> layer shape = ', units['P0'])\n",
    "    for i in range(1, len(hpar['layer_conv_filter'])+1): #CONV2D -> RELU -> MAXPOOL LAYERS\n",
    "        print('>> layer shape = ', units['Z%d'%i])\n",
    "        print('>> layer shape = ', units['A%d'%i])\n",
    "        print('>> layer shape = ', units['P%d'%i])\n",
    "    print('>> layer shape = ', units['F0'])\n",
    "    for i in range(1, len(hpar['hlayer_fully_connected']) + 2): # FULLY-CONNECTED LAYERS\n",
    "        print('>> layer shape = ', units['F%d'%i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(labels, C):\n",
    "    C = tf.constant(C, name='C') # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels.squeeze(), C, axis=0)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    return one_hot\n",
    "def model(X_train, Y_train, X_test, Y_test, hpar):\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\n",
    "    seed = 3                                          # to keep results consistent (numpy seed)\n",
    "    (m, n_H0, n_W0, n_C0) = X_train.shape             \n",
    "    n_y = Y_train.shape[1]                            \n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    X, Y = create_placeholders(n_H0, n_W0, n_C0, n_y) # Create Placeholders of the correct shape\n",
    "    parameters = initialize_parameters(n_C0, hpar) # Initialize parameters\n",
    "    Z3, units = forward_propagation(X, n_y, parameters, hpar) # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    print_units(parameters, units, hpar)\n",
    "    cost = compute_cost(Z3, Y) # Cost function: Add cost function to tensorflow graph\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=hpar['learning_rate']).minimize(cost) # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\n",
    "    init = tf.global_variables_initializer() # Initialize all the variables globally\n",
    "    with tf.Session() as sess: # Start the session to compute the tensorflow graph\n",
    "        sess.run(init) # Run the initialization\n",
    "        for epoch in range(hpar['num_epochs']): # Do the training loop\n",
    "            minibatch_cost = 0.\n",
    "            num_minibatches = int(m / hpar['minibatch_size']) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, hpar['minibatch_size'], seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch # Select a minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                _ , temp_cost = sess.run([optimizer,cost], feed_dict={X:minibatch_X, Y:minibatch_Y}) # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\n",
    "                minibatch_cost += temp_cost / num_minibatches\n",
    "            costs.append(minibatch_cost)\n",
    "            # Print the cost every epoch\n",
    "            if hpar['print_cost'] > 0 and epoch % hpar['print_cost'] == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "        # plot the cost\n",
    "        plt.plot(np.squeeze(costs))\n",
    "        plt.ylabel('cost')\n",
    "        plt.xlabel('iterations (per tens)')\n",
    "        plt.title(\"Learning rate =\" + str(hpar['learning_rate']))\n",
    "        plt.show()\n",
    "        # Calculate the correct predictions\n",
    "        predict_op = tf.argmax(Z3, 1)\n",
    "        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\n",
    "        # Calculate accuracy on the test set\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print(accuracy)\n",
    "        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\n",
    "        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\n",
    "        print(\"Train Accuracy:\", train_accuracy)\n",
    "        print(\"Test Accuracy:\", test_accuracy)\n",
    "        return train_accuracy, test_accuracy, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training examples = 1080\n",
      "number of test examples = 120\n",
      "X_train shape: (1080, 64, 64, 3)\n",
      "Y_train shape: (1080, 6)\n",
      "X_test shape: (120, 64, 64, 3)\n",
      "Y_test shape: (120, 6)\n",
      ">> param shape =  <tf.Variable 'W1:0' shape=(4, 4, 3, 8) dtype=float32_ref>\n",
      ">> param shape =  <tf.Variable 'W2:0' shape=(2, 2, 8, 16) dtype=float32_ref>\n",
      ">> layer shape =  Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)\n",
      ">> layer shape =  Tensor(\"Conv2D:0\", shape=(?, 64, 64, 8), dtype=float32)\n",
      ">> layer shape =  Tensor(\"Relu:0\", shape=(?, 64, 64, 8), dtype=float32)\n",
      ">> layer shape =  Tensor(\"MaxPool:0\", shape=(?, 8, 8, 8), dtype=float32)\n",
      ">> layer shape =  Tensor(\"Conv2D_1:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      ">> layer shape =  Tensor(\"Relu_1:0\", shape=(?, 8, 8, 16), dtype=float32)\n",
      ">> layer shape =  Tensor(\"MaxPool_1:0\", shape=(?, 2, 2, 16), dtype=float32)\n",
      ">> layer shape =  Tensor(\"Flatten/flatten/Reshape:0\", shape=(?, 64), dtype=float32)\n",
      ">> layer shape =  Tensor(\"fully_connected/BiasAdd:0\", shape=(?, 6), dtype=float32)\n",
      "Cost after epoch 0: 1.921722\n",
      "Cost after epoch 5: 1.612302\n",
      "Cost after epoch 10: 0.840686\n",
      "Cost after epoch 15: 0.661784\n",
      "Cost after epoch 20: 0.581210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-3511bfa2d87e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mhpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'minibatch_size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m64\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mhpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'print_cost'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;31m# print epoch step; no print when it is zero\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhpar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-f0032c9725f9>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X_train, Y_train, X_test, Y_test, hpar)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[1;31m# Select a minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[1;31m# IMPORTANT: The line that runs the graph on a minibatch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mtemp_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_Y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[0mminibatch_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtemp_cost\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnum_minibatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mcosts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mminibatch_cost\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_SINGS_data(dirpath = './datasets/coursera_dnn4_week1_Convolutional Model/datasets/'):\n",
    "    import h5py\n",
    "    train_dataset = h5py.File(dirpath+'train_signs.h5', \"r\")\n",
    "    X_train_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    Y_train_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    test_dataset = h5py.File(dirpath+'test_signs.h5', \"r\")\n",
    "    X_test_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    Y_test_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    Y_train_orig = Y_train_orig.reshape((1, Y_train_orig.shape[0]))\n",
    "    Y_test_orig = Y_test_orig.reshape((1, Y_test_orig.shape[0]))\n",
    "    #-------------------------------------------------------\n",
    "    X_train = X_train_orig/255. # Normalize image vectors\n",
    "    X_test = X_test_orig/255.\n",
    "    Y_train = convert_to_one_hot(Y_train_orig, len(classes)).T # Convert training and test labels to one hot matrices\n",
    "    Y_test = convert_to_one_hot(Y_test_orig, len(classes)).T\n",
    "    return X_train, Y_train, X_test, Y_test, classes\n",
    "if True:\n",
    "    X_train, Y_train, X_test, Y_test, classes = load_SINGS_data()\n",
    "    print (\"number of training examples = \" + str(X_train.shape[0]))\n",
    "    print (\"number of test examples = \" + str(X_test.shape[0]))\n",
    "    print (\"X_train shape: \" + str(X_train.shape))\n",
    "    print (\"Y_train shape: \" + str(Y_train.shape))\n",
    "    print (\"X_test shape: \" + str(X_test.shape))\n",
    "    print (\"Y_test shape: \" + str(Y_test.shape))    \n",
    "    #-----------------------------------------------------------------\n",
    "    hpar = {}\n",
    "    hpar['layer_conv_filter'] = [[4,4,8], [2,2,16]] # [height, width, channel]\n",
    "    hpar['layer_conv_stride'] = [[1,1], [1,1]] # [height, width]\n",
    "    hpar['layer_conv_padding'] = ['SAME', 'SAME']\n",
    "    hpar['layer_pool_filter'] = [[8,8], [4,4]] # [height, width]\n",
    "    hpar['layer_pool_stride'] = [[8,8], [4,4]] # [height, width]\n",
    "    hpar['layer_pool_padding'] = ['SAME', 'SAME']\n",
    "    hpar['hlayer_fully_connected'] = []\n",
    "    hpar['learning_rate'] = 0.009\n",
    "    hpar['num_epochs'] = 100\n",
    "    hpar['minibatch_size'] = 64\n",
    "    hpar['print_cost'] = 5 # print epoch step; no print when it is zero\n",
    "    _, _, parameters = model(X_train, Y_train, X_test, Y_test, hpar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cost after epoch 0: 1.921722\n",
    "Cost after epoch 5: 1.612302\n",
    "Cost after epoch 10: 0.840686\n",
    "Cost after epoch 15: 0.661784\n",
    "Cost after epoch 20: 0.581210\n",
    "Cost after epoch 25: 0.471013\n",
    "Cost after epoch 30: 0.410983\n",
    "Cost after epoch 35: 0.450856\n",
    "Cost after epoch 40: 0.366354\n",
    "Cost after epoch 45: 0.337586\n",
    "Cost after epoch 50: 0.377859\n",
    "Cost after epoch 55: 0.288762\n",
    "Cost after epoch 60: 0.270143\n",
    "Cost after epoch 65: 0.280989\n",
    "Cost after epoch 70: 0.255999\n",
    "Cost after epoch 75: 0.251713\n",
    "Cost after epoch 80: 0.229010\n",
    "Cost after epoch 85: 0.252714\n",
    "Cost after epoch 90: 0.199755\n",
    "Cost after epoch 95: 0.210600\n",
    "Tensor(\"Mean_1:0\", shape=(), dtype=float32)\n",
    "Train Accuracy: 0.90463\n",
    "Test Accuracy: 0.8\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
