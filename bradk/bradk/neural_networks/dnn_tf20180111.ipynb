{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x,None), name='X')\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y,None), name='Y')\n",
    "    return X, Y\n",
    "def initialize_parameters(layerdims, seed = 1):\n",
    "    tf.set_random_seed(seed)                   # so that your \"random\" numbers match ours\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layerdims)):\n",
    "        parameters[\"W%d\"%i] = tf.get_variable(\"W%d\"%i, [layerdims[i],layerdims[i-1]], initializer = tf.contrib.layers.xavier_initializer(seed = seed))\n",
    "        parameters[\"b%d\"%i] = tf.get_variable(\"b%d\"%i, [layerdims[i],1], initializer = tf.zeros_initializer())\n",
    "    return parameters\n",
    "def forward_propagation(X, parameters, hpar):\n",
    "    L = len(parameters) // 2 # 2 -> there are two groups: W's and b's\n",
    "    units = {}\n",
    "    units['Z1'] = tf.add(tf.matmul(parameters['W1'],X),parameters['b1'])\n",
    "    for i in range(2, L+1):\n",
    "        if hpar['activation_hidden_layers'] == 'sigmoid':\n",
    "            units['A'+str(i-1)] = tf.nn.sigmoid(units['Z'+str(i-1)]) # sigmoidfunction\n",
    "        elif hpar['activation_hidden_layers'] == 'tanh':\n",
    "            units['A'+str(i-1)] = tf.nn.tanh(units['Z'+str(i-1)]) # tanh function\n",
    "        elif hpar['activation_hidden_layers'] == 'relu':\n",
    "            units['A'+str(i-1)] = tf.nn.relu(units['Z'+str(i-1)]) # ReLU function\n",
    "        units['Z'+str(i)] = tf.add(tf.matmul(parameters['W'+str(i)], units['A'+str(i-1)]), parameters['b'+str(i)])\n",
    "    ZL = units['Z'+str(L)]\n",
    "    return ZL # output of the last LINEAR unit\n",
    "def compute_cost(ZL, Y, hpar):\n",
    "    logits = tf.transpose(ZL) # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    labels = tf.transpose(Y) # to fit the tensorflow requirement for tf.nn.softmax_cross_entropy_with_logits(...,...)\n",
    "    if hpar['activation_last_layer'] == 'sigmoid':\n",
    "        cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    elif hpar['activation_last_layer'] == 'softmax':\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    return cost\n",
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0): # modified date: 01/15/2018\n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = np.random.permutation(m)\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation]\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = max(1, m // mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:, (k*mini_batch_size):((k+1)*mini_batch_size)]\n",
    "        mini_batch_Y = shuffled_Y[:, (k*mini_batch_size):((k+1)*mini_batch_size)]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    if (m >= mini_batch_size) and (m % mini_batch_size != 0): # Handling the end case (last mini-batch < mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[:, (num_complete_minibatches*mini_batch_size):m]\n",
    "        mini_batch_Y = shuffled_Y[:, (num_complete_minibatches*mini_batch_size):m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches\n",
    "def normalize_X_Y(X_train0, Y_train0, X_test0, Y_test0, hpar): # modified date: 01/11/2018\n",
    "    if hpar['input_normalization'] == True:\n",
    "        Xu = np.mean(X_train0, axis = 1, keepdims = True)\n",
    "        Xs = np.std(X_train0, axis = 1, keepdims = True)\n",
    "    else:\n",
    "        Xu = np.zeros((X_train0.shape[0],1))\n",
    "        Xs = np.full((X_train0.shape[0],1), 1.0)\n",
    "    X_train = (X_train0 - Xu) / Xs\n",
    "    Y_train = np.int64(Y_train0 > 0.5)\n",
    "    X_test = (X_test0 - Xu) / Xs\n",
    "    if len(Y_test0) != 0:\n",
    "        Y_test = np.int64(Y_test0 > 0.5)\n",
    "    else:\n",
    "        Y_test = []\n",
    "    return X_train, Xu, Xs, Y_train, X_test, Y_test\n",
    "def denormalize_W1_b1(WW1, bb1, Xu, Xs): # modified date: 01/11/2018\n",
    "    W1 = WW1 / Xs.T\n",
    "    b1 = bb1 - np.dot(WW1, Xu / Xs)\n",
    "    return W1, b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_to_one_hot(labels, C):\n",
    "    C = tf.constant(C, name='C') # Create a tf.constant equal to C (depth), name it 'C'. (approx. 1 line)\n",
    "    one_hot_matrix = tf.one_hot(labels.squeeze(), C, axis=0)\n",
    "    sess = tf.Session()\n",
    "    one_hot = sess.run(one_hot_matrix)\n",
    "    sess.close()\n",
    "    return one_hot\n",
    "def train(X_train0, Y_train0, X_test0, Y_test0, hpar):\n",
    "    print('> Deep Neural Network (tensorflow) started ...')\n",
    "    X_train, Xu, Xs, Y_train, X_test, Y_test = normalize_X_Y(X_train0, Y_train0, X_test0, Y_test0, hpar)\n",
    "    print('>> X_train shape = '+str(X_train.shape))\n",
    "    print('>> Y_train shape = '+str(Y_train.shape))\n",
    "    print('>> X_test shape = '+str(X_test.shape if len(X_test)!=0 else X_test))\n",
    "    print('>> Y_test shape = '+str(Y_test.shape if len(Y_test)!=0 else Y_test))\n",
    "    print(\">> the number of training data = \" + str(X_train.shape[1]))\n",
    "    print(\">> the number of test data = \" + str(X_test.shape[1] if len(X_test)!=0 else X_test))\n",
    "    layerdims = [X_train.shape[0]] + hpar['hidden_layer_dims'] + [Y_train.shape[0]]\n",
    "    print('>> the number of units of layers = '+str(layerdims))\n",
    "    print(\">> the number of epochs = \" + str(hpar['num_epochs']))\n",
    "    print(\">> learning rate = \" + str(hpar['learning_rate']))\n",
    "    print(\">> activation function of hidden layers = \" + hpar['activation_hidden_layers'])\n",
    "    print(\">> activation function of the last layer = \" + hpar['activation_last_layer'])\n",
    "    print(\">> mini batch size = \" + str(hpar['minibatch_size']))\n",
    "    print(\">> optimizer = \" + hpar['optimizer'])\n",
    "    print(\">> input normalization = \" + str(hpar['input_normalization']))\n",
    "    print(\">> global random seed = \" + str(hpar['global_random_seed']))\n",
    "    #------------------------------------------------------------------------------------\n",
    "    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\n",
    "    tf.set_random_seed(hpar['global_random_seed'])                             # to keep consistent results\n",
    "#    seed = 3                                          # to keep consistent results\n",
    "    seed = hpar['global_random_seed']\n",
    "    (n_x, m) = X_train.shape                          # (n_x: input size, m : number of examples in the train set)\n",
    "    n_y = Y_train.shape[0]                            # n_y : output size\n",
    "    costs = []                                        # To keep track of the cost\n",
    "    \n",
    "    X, Y = create_placeholders(n_x, n_y) # Create Placeholders of shape (n_x, n_y)\n",
    "    parameters = initialize_parameters(layerdims, hpar['global_random_seed']) # Initialize parameters\n",
    "    ZL = forward_propagation(X, parameters, hpar) # Forward propagation: Build the forward propagation in the tensorflow graph\n",
    "    cost = compute_cost(ZL, Y, hpar) # Cost function: Add cost function to tensorflow graph\n",
    "    if hpar['optimizer'] == 'gradient descent':\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = hpar['learning_rate']).minimize(cost) # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    elif hpar['optimizer'] == 'momentum':\n",
    "        optimizer = tf.train.MomentumOptimizer(learning_rate = hpar['learning_rate']).minimize(cost) # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    elif hpar['optimizer'] == 'adam':\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate = hpar['learning_rate']).minimize(cost) # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer.\n",
    "    init = tf.global_variables_initializer() # Initialize all the variables\n",
    "    \n",
    "    with tf.Session() as sess: # Start the session to compute the tensorflow graph\n",
    "        sess.run(init) # Run the initialization\n",
    "        tic = time.time()\n",
    "#        for epoch in range(0, hpar['num_epochs']): # Do the training loop        \n",
    "        for epoch in range(1, hpar['num_epochs']+1): # Do the training loop\n",
    "            epoch_cost = 0.                       # Defines a cost related to an epoch\n",
    "            num_minibatches = max(1, int(m / hpar['minibatch_size'])) # number of minibatches of size minibatch_size in the train set\n",
    "            seed = seed + 1\n",
    "            minibatches = random_mini_batches(X_train, Y_train, hpar['minibatch_size'], seed)\n",
    "\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch # Select a minibatch\n",
    "                # IMPORTANT: The line that runs the graph on a minibatch.\n",
    "                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict={X:minibatch_X, Y:minibatch_Y}) # Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\n",
    "#                epoch_cost += minibatch_cost / num_minibatches\n",
    "                epoch_cost += minibatch_cost * (minibatch_X.shape[1] / m)\n",
    "            costs.append(epoch_cost)\n",
    "            # Print the cost every epoch\n",
    "#            if hpar['print_cost'] > 0 and epoch % hpar['print_cost'] == 0:\n",
    "            if hpar['print_cost'] > 0 and ((epoch == 1) or (epoch % hpar['print_cost'] == 0) or (epoch == hpar['num_epochs'])):\n",
    "                toc = time.time()\n",
    "                print (\">> epoch =  %i; cost = %f; time = %f sec\" % (epoch, epoch_cost, toc-tic))\n",
    "                tic = time.time()\n",
    "        ## plot the cost\n",
    "        #plt.plot(np.squeeze(costs))\n",
    "        #plt.ylabel('cost')\n",
    "        #plt.xlabel('iterations (per tens)')\n",
    "        #plt.title(\"Learning rate =\" + str(hpar['learning_rate']))\n",
    "        #plt.show()\n",
    "        # lets save the parameters in a variable\n",
    "        parameters = sess.run(parameters)\n",
    "        #print(\">> Parameters have been trained!\")\n",
    "        # Calculate the correct predictions\n",
    "        #correct_prediction = tf.equal(tf.argmax(ZL), tf.argmax(Y))\n",
    "        ## Calculate accuracy on the test set\n",
    "        #accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        #print(\">> Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        #print(\">> Test Accuracy:\", accuracy.eval({X: X_test, Y: Y_test}))\n",
    "    Yp_train, accuracy_train = predict(X_train, Y_train, parameters, hpar)\n",
    "    Yp_test, accuracy_test = predict(X_test, Y_test, parameters, hpar)\n",
    "    print(\">> Train Accuracy:\" + str(accuracy_train))\n",
    "    print(\">> Test Accuracy:\" + str(accuracy_test))\n",
    "    parameters['W1'], parameters['b1'] = denormalize_W1_b1(parameters['W1'], parameters['b1'], Xu, Xs) # denormalization\n",
    "    print('> Deep Neural Network (tensorflow) ended ...')\n",
    "    dout = {'parameters': parameters,\n",
    "            'costs': costs,\n",
    "            'Yp_train': Yp_train,\n",
    "            'Yp_test': Yp_test}\n",
    "    return dout\n",
    "def predict(X_data, Y_data, parameters, hpar):\n",
    "    para = {}\n",
    "    for i in range(1, (len(parameters) // 2) + 1):\n",
    "        para['W%d'%i] = tf.convert_to_tensor(parameters['W%d'%i], dtype = tf.float32)\n",
    "        para['b%d'%i] = tf.convert_to_tensor(parameters['b%d'%i], dtype = tf.float32)\n",
    "    xx = tf.constant(X_data, dtype = tf.float32)\n",
    "    zl = forward_propagation(xx, para, hpar)\n",
    "    sess = tf.Session()\n",
    "    if hpar['activation_last_layer'] == 'sigmoid':\n",
    "        al = tf.nn.sigmoid(zl)\n",
    "        AL = sess.run(al)\n",
    "        Yp = np.int64(AL > 0.5)\n",
    "        YY = np.int64(Y_data > 0.5)\n",
    "        accuracy = np.mean(np.all(YY == Yp, axis=0))\n",
    "    elif hpar['activation_last_layer'] == 'softmax':\n",
    "        ZL = sess.run(zl)\n",
    "        YL = np.argmax(ZL, axis = 0)\n",
    "        YY = np.argmax(Y_data, axis = 0)\n",
    "        accuracy = np.mean(np.int64(YY == YL))\n",
    "        Yp = convert_to_one_hot(YL, Y_data.shape[0])\n",
    "    sess.close()\n",
    "    return Yp, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Deep Neural Network (tensorflow) started ...\n",
      ">> X_train shape = (12288, 1080)\n",
      ">> Y_train shape = (6, 1080)\n",
      ">> X_test shape = (12288, 120)\n",
      ">> Y_test shape = (6, 120)\n",
      ">> the number of training data = 1080\n",
      ">> the number of test data = 120\n",
      ">> the number of units of layers = [12288, 25, 12, 6]\n",
      ">> the number of epochs = 1500\n",
      ">> learning rate = 0.0001\n",
      ">> activation function of hidden layers = relu\n",
      ">> activation function of the last layer = softmax\n",
      ">> mini batch size = 32\n",
      ">> optimizer = adam\n",
      ">> input normalization = False\n",
      ">> global random seed = 1\n",
      ">> epoch =  1; cost = 1.794330; time = 1.087062 sec\n",
      ">> epoch =  100; cost = 1.077817; time = 91.802573 sec\n",
      ">> epoch =  200; cost = 0.732875; time = 94.641399 sec\n",
      ">> epoch =  300; cost = 0.562160; time = 91.449497 sec\n",
      ">> epoch =  400; cost = 0.373987; time = 94.588391 sec\n",
      ">> epoch =  500; cost = 0.253109; time = 92.190963 sec\n",
      ">> epoch =  600; cost = 0.191426; time = 90.322149 sec\n",
      ">> epoch =  700; cost = 0.124159; time = 90.284715 sec\n",
      ">> epoch =  800; cost = 0.086077; time = 133.292911 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-c7d636892ca0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mhpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_normalization'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mhpar\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'global_random_seed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m     \u001b[0mdout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhpar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m     \u001b[1;31m#-----------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mYp_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parameters'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhpar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-c75d1a831f74>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(X_train0, Y_train0, X_test0, Y_test0, hpar)\u001b[0m\n\u001b[0;32m     59\u001b[0m                 \u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_Y\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mminibatch\u001b[0m \u001b[1;31m# Select a minibatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[1;31m# IMPORTANT: The line that runs the graph on a minibatch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0m_\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mminibatch_Y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Run the session to execute the \"optimizer\" and the \"cost\", the feedict should contain a minibatch for (X,Y).\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;31m#                epoch_cost += minibatch_cost / num_minibatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mepoch_cost\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mminibatch_cost\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mminibatch_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def load_SINGS_data(dirpath = './datasets/coursera_dnn2_week3_Tensorflow/datasets/'):\n",
    "    import h5py\n",
    "    train_dataset = h5py.File(dirpath+'train_signs.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "    test_dataset = h5py.File(dirpath+'test_signs.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    #-------------------------------------------------------\n",
    "    X_train_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T # Flatten the training and test images\n",
    "    X_test_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "    X_train = X_train_flatten/255. # Normalize image vectors\n",
    "    X_test = X_test_flatten/255.\n",
    "    Y_train = convert_to_one_hot(train_set_y_orig, len(classes)) # Convert training and test labels to one hot matrices\n",
    "    Y_test = convert_to_one_hot(test_set_y_orig, len(classes))\n",
    "    return X_train, Y_train, X_test, Y_test, classes\n",
    "if True:\n",
    "    X_train, Y_train, X_test, Y_test, classes = load_SINGS_data()\n",
    "    #-----------------------------------------------------------------\n",
    "    hpar = {}\n",
    "    hpar['hidden_layer_dims'] = [25,12]\n",
    "    hpar['activation_hidden_layers'] = ['sigmoid', 'tanh', 'relu'][2]\n",
    "    hpar['activation_last_layer'] = ['sigmoid', 'softmax'][1]\n",
    "    hpar['learning_rate'] = 0.0001\n",
    "    hpar['num_epochs'] = 1500\n",
    "    hpar['minibatch_size'] = 32\n",
    "    hpar['optimizer'] = [\"gradient descent\", \"momentum\", \"adam\"][2]\n",
    "    hpar['print_cost'] = 100 # print epoch step; no print when it is zero\n",
    "    hpar['input_normalization'] = False\n",
    "    hpar['global_random_seed'] = 1\n",
    "    dout = train(X_train, Y_train, X_test, Y_test, hpar)\n",
    "    #-----------------------------------------------------------------\n",
    "    Yp_train, accuracy_train = predict(X_train, Y_train, dout['parameters'], hpar)\n",
    "    Yp_test, accuracy_test = predict(X_test, Y_test, dout['parameters'], hpar)\n",
    "    print(\"- Train Accuracy:\" + str(accuracy_train))\n",
    "    print(\"- Test Accuracy:\" + str(accuracy_test))      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ">> epoch =  1; cost = 1.794330; time = 0.887050 sec\n",
    ">> epoch =  100; cost = 1.077817; time = 91.994224 sec\n",
    ">> epoch =  200; cost = 0.732875; time = 89.827482 sec\n",
    ">> epoch =  300; cost = 0.562160; time = 86.932952 sec\n",
    ">> epoch =  400; cost = 0.373987; time = 87.098551 sec\n",
    ">> epoch =  500; cost = 0.253109; time = 88.382034 sec\n",
    ">> epoch =  600; cost = 0.191426; time = 89.230349 sec\n",
    ">> epoch =  700; cost = 0.124159; time = 86.080905 sec\n",
    ">> epoch =  800; cost = 0.086077; time = 87.708269 sec\n",
    ">> epoch =  900; cost = 0.073558; time = 87.350744 sec\n",
    ">> epoch =  1000; cost = 0.046608; time = 90.128737 sec\n",
    ">> epoch =  1100; cost = 0.038868; time = 88.938455 sec\n",
    ">> epoch =  1200; cost = 0.029329; time = 87.346982 sec\n",
    ">> epoch =  1300; cost = 0.104658; time = 89.861315 sec\n",
    ">> epoch =  1400; cost = 0.021424; time = 90.816179 sec\n",
    ">> epoch =  1500; cost = 0.018487; time = 91.619495 sec\n",
    ">> Train Accuracy:0.994444444444\n",
    ">> Test Accuracy:0.766666666667\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
